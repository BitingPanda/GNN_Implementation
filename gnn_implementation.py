# -*- coding: utf-8 -*-
"""GNN Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JfV3yVspRYlL66uos70fJONfKcEvjb6v

# A Graph Neural Network Implementation

This is a Graph Neural Network Model that trains from Planetoid citation network dataset which is a part of torch.geometri.dataset and predicts missing labels

#Creating the Actual Model

### Pre-requisites

Installing Pytorch
"""

!pip install -q torch

"""Then I installed torch-scatter and torch-sparse. After that, I installed pytorch_geometric's too."""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# import torch
# os.environ['TORCH'] = torch.__version__
# os.environ['PYTHONWARNINGS'] = "ignore"
# !pip install torch-scatter -f https:://data.pyg.org/whl/torch-${TORCH}.html
# !pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install git+https://github.com/pyg-team/pytorch_geometric.git

"""### Importing Dataset

I used **Planetoid** which is a citation network dataset from *Cora*, *CiteSeer* and *PubMed* from the ["Revisiting Semi-Supervised Learning with Graph Embeddings"](https://arxiv.org/abs/1603.08861)

On that, the nodes are documents with 1433 dimentional bag of words feature vectors, an edges are citation links between research papers. It also has 7 classes.

I ingested the Planetoid Cora dataset, and row normalise the bag of words input features.

Afterwords, I analysed the dataset and the first graph object will be
"""

from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures

dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())

"""From Planetoid, I took the Cora datasets

"""

print(f'Dataset: {dataset}:')
print('======================')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')

"""The Cora dataset has
- 1 Graph
- 2708 nodes,
- 10,556 edges,
- 1433 features,
- 7 classes.

The first object has 2708 train, validation, and test masks.

I used these masks to train and evaluate the model.

To get The First Object details
"""

data = dataset[0]  # Get the first graph object.
print(data)

"""Then, I used Graph Convolutional Network model structure that contains
-  2 GCNConv layers Rectified Linear Unit (Relu) activation
- dropout rate of 0.5. The model consists of 16 hidden channels

### Creating the Actual Model
"""

from torch_geometric.nn import GCNConv
import torch.nn.functional as F

class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super().__init__()
        torch.manual_seed(1234567)
        self.conv1 = GCNConv(dataset.num_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = x.relu()
        x = F.dropout(x, p=0.5, training=self.training)
        x = self.conv2(x, edge_index)
        return x

model = GCN(hidden_channels=16)
print(model)

"""# Visualising The Untrained Model

Now Visualising the Untrained GCN network

Will be using
- sklearn.manifold.TSNE
- matplotlib.pyplot.
  
It will plot a 7 dimension node embedding a 2D scatter plot
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE


def visualise(h, color):
  z = TSNE(n_components = 2).fit_transform(h.detach().cpu().numpy())


  plt.figure(figsize=(10,10))
  plt.xticks([])
  plt.yticks([])


  plt.scatter(z[:,0], z[:,1], s=70, c=color, cmap="Set2")
  plt.show()

"""The model have to be evaluated before training data to the untrained model to visualise various nodes and categories"""

model.eval()

out = model(data.x, data.edge_index)
visualise(out, color=data.y)

"""# Trainning The Graph Neural Network

The Model will be trained on 100 Epochs *(A cycle of dataset was fed to the model)*
- Using Adam Optimisation
- Cross-Entropy Loss Function
"""

model = GCN(hidden_channels=16)
optimiser = torch.optim.Adam(model.parameters(),lr=0.01,weight_decay=5e-4)
criterion = torch.nn.CrossEntropyLoss()

"""### Creating Training Function

The Train Function has
- Clear Gradient
- Performed a single forwarding pass
- Calculate the loss using trainning nodes
- Calculate Gradient and update the parameters
"""

def train():
    model.train()
    optimiser.zero_grad()
    out = model(data.x,data.edge_index)
    loss= criterion(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimiser.step()
    return loss

"""### Creating Test Function

The Test Function
- Predicted Node class
- Extracted class label with the highest probability
- Checked how many values have been predicted correctly
- Creating Accuracy ratio using a sum of correct prediction divided by a total number of nodes
"""

def test():
    model.eval()
    out = model(data.x, data.edge_index)
    pred = out.argmax(dim=1)
    test_correct = pred[data.test_mask] == data.y [data.test_mask]
    test_acc = int(test_correct.sum())/int(data.test_mask.sum())
    return test_acc

"""### Training Loop

Training the model
"""

for epoch in range(1, 101):
    loss = train()
    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')

"""Model Evaluation

# Accuracy Test
"""

test_acc = test()
print(f'Test Accuracy: {test_acc:4f}')

"""# Visualising The Trained Model

Visualising the model
"""

model.eval()
out = model(data.x, data.edge_index)
visualise(out, color=data.y)